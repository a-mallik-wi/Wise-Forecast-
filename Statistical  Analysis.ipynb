{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3ca845-b446-4adf-83ff-c8aebd457189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain-community databricks-sql-connector databricks_langchain openai databricks-sqlalchemy~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8adfc9c-c8e1-4963-acb9-3b449357c7b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a592040-1844-4e5d-a596-dbcfea7ced78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain import OpenAI\n",
    "from databricks_langchain import ChatDatabricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660c7c8a-b252-4049-a6aa-3c3b8dd3de77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = spark.read.table(\"forecast.schema1.calendar\") #Replace with your catalog,schema and table name\n",
    "    print(\"Successfully read the table!\")\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading table directly with Spark: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1b0ad6-e863-4952-95e5-fbe1964f96fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"forecast\"\n",
    "schema = \"schema1\"\n",
    "\n",
    "from langchain.sql_database import SQLDatabase\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"TimeSeriesAnalysis\").getOrCreate()\n",
    "\n",
    "# 1. Initialize LangChain's SQLDatabase (ONLY if using the agent)\n",
    "try:\n",
    "    db = SQLDatabase.from_databricks(catalog=\"forecast\", schema=\"schema1\", engine_args={\"pool_pre_ping\": True})\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LangChain DB: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    query = \"SHOW TABLES\"\n",
    "    tables_list = db.query(query) \n",
    "    tables = {}\n",
    "\n",
    "    for table_info in tables_list:\n",
    "        table_name = table_info['tableName']  \n",
    "        df = db.get_table(table_name)\n",
    "        tables[table_name] = df\n",
    "\n",
    "    print(f\"Tables extracted: {tables.keys()}\")  \n",
    "except Exception as e:\n",
    "    print(f\"Error extracting tables: {e}\")\n",
    "\n",
    "\n",
    "def get_available_tables(catalog, schema):\n",
    "    try:\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "        table_names = [row.tableName for row in tables if row.tableName != 'small_sales' and row.tableName != '_sqldf']\n",
    "        return table_names\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving tables from {catalog}.{schema}: {e}\")\n",
    "        return []\n",
    "\n",
    "tables = {}\n",
    "\n",
    "try:\n",
    "    table_names = get_available_tables(catalog=\"forecast\", schema=\"schema1\")\n",
    "    \n",
    "    for table_name in table_names:\n",
    "        try:\n",
    "            # Read the table using Spark\n",
    "            df = spark.read.table(f\"forecast.schema1.{table_name}\")\n",
    "            tables[table_name] = df\n",
    "            print(f\"Successfully read table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading table {table_name}: {e}\")\n",
    "    \n",
    "    print(f\"Tables extracted: {tables.keys()}\")\n",
    "\n",
    "    merged_df = None\n",
    "\n",
    "    for table_name, df in tables.items():\n",
    "        pandas_df = df.toPandas()\n",
    "\n",
    "        if merged_df is None:\n",
    "            merged_df = pandas_df\n",
    "        else:\n",
    "            merged_df = pd.concat([merged_df, pandas_df], ignore_index=True)\n",
    "    \n",
    "    print(\"Successfully merged all tables\")\n",
    "    print(merged_df.head())  \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting or merging tables: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def get_columns_from_db(catalog, schema, tables):\n",
    "    schema_col = {}\n",
    "    for table in tables:\n",
    "        try:\n",
    "            cols = spark.table(f\"{catalog}.{schema}.{table}\").columns  \n",
    "            schema_col[table] = cols\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving columns for {catalog}.{schema}.{table}: {e}\")\n",
    "            schema_col[table] = [] \n",
    "    return schema_col\n",
    "\n",
    "  \n",
    "table_names = get_available_tables(catalog, schema)\n",
    "\n",
    "columns_schema = get_columns_from_db(catalog, schema, table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc100708-d7e5-4ec2-8f08-b85c98de9755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain_databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cf7210-8903-45f7-9ada-4760251d72cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf, grangercausalitytests\n",
    "from pyspark.sql import SparkSession\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TimeSeriesAnalysis\").getOrCreate()\n",
    "\n",
    "catalog = \"forecast\"\n",
    "schema = \"schema1\"\n",
    "\n",
    "def get_available_tables(catalog, schema):\n",
    "    try:\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "        return [row.tableName for row in tables if row.tableName != '_sqldf']\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving tables from {catalog}.{schema}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_columns_from_db(catalog, schema, tables):\n",
    "    schema_col = {}\n",
    "    for table in tables:\n",
    "        try:\n",
    "            cols = spark.table(f\"{catalog}.{schema}.{table}\").columns  \n",
    "            schema_col[table] = cols\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving columns for {catalog}.{schema}.{table}: {e}\")\n",
    "            schema_col[table] = []\n",
    "    return schema_col\n",
    "\n",
    "tables = {}\n",
    "table_names = get_available_tables(catalog, schema)\n",
    "\n",
    "for table_name in table_names:\n",
    "    try:\n",
    "        df = spark.read.table(f\"{catalog}.{schema}.{table_name}\").toPandas()\n",
    "        tables[table_name] = df\n",
    "        print(f\"Successfully loaded table: {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading table {table_name}: {e}\")\n",
    "\n",
    "columns_schema = get_columns_from_db(catalog, schema, table_names)\n",
    "\n",
    "def compute_statistical_relation(df, target_column):\n",
    "    results = []\n",
    "    target_data = df[target_column].dropna()  \n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col == target_column:\n",
    "            continue\n",
    "        \n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            col_data = df[col].dropna() \n",
    "            \n",
    "           \n",
    "            aligned_data = pd.concat([col_data, target_data], axis=1).dropna()\n",
    "            \n",
    "            if aligned_data.shape[0] < 2:  \n",
    "                continue\n",
    "            \n",
    "            corr, _ = stats.pearsonr(aligned_data[col], aligned_data[target_column])\n",
    "            score = abs(corr)\n",
    "        else:\n",
    "            contingency_table = pd.crosstab(df[col], target_data)\n",
    "            chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "            score = chi2\n",
    "        \n",
    "        results.append((col, score))\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def forecasting_statistical_tests(df, target_column):\n",
    "    results = {}\n",
    "    target_data = df[target_column].dropna()\n",
    "\n",
    "    if df[target_column].dtype not in ['int64', 'float64']:\n",
    "        return \"Target column must be numerical for forecasting tests.\"\n",
    "\n",
    "    results[\"Correlation Matrix\"] = df.corr()\n",
    "    results[\"ACF\"] = acf(target_data, nlags=20)\n",
    "    results[\"PACF\"] = pacf(target_data, nlags=20)\n",
    "    results[\"Augmented Dickey-Fuller Test\"] = adfuller(target_data)\n",
    "    results[\"Ljung-Box Test\"] = sm.stats.acorr_ljungbox(target_data, lags=[10])\n",
    "\n",
    "    granger_results = {}\n",
    "    for col in df.columns:\n",
    "        if col != target_column and df[col].dtype in ['int64', 'float64']:\n",
    "            try:\n",
    "                test_result = grangercausalitytests(df[[target_column, col]].dropna(), maxlag=5, verbose=False)\n",
    "                granger_results[col] = test_result\n",
    "            except:\n",
    "                continue\n",
    "    results[\"Granger Causality Test\"] = granger_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=800,\n",
    ")\n",
    "\n",
    "def extract_target_from_query(user_query, llm, table_names, columns_schema):\n",
    "    prompt_template = \"\"\"\n",
    "    Given the following table names and schema, determine the most relevant numerical target column for forecasting future trends.\n",
    "    \n",
    "    Tables: {tables}\n",
    "    Schema: {columns_schema}\n",
    "    Query: {query}\n",
    "\n",
    "    Return ONLY the best target column name as a single word. Do NOT return extra text.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt_template.format(\n",
    "        tables=table_names, columns_schema=columns_schema, query=user_query\n",
    "    )).content.strip()  \n",
    "\n",
    "    print(f\"ðŸ” LLM Response: {response}\") \n",
    "\n",
    "    for table_name, columns in columns_schema.items():\n",
    "        if response in columns:\n",
    "            return table_name, response\n",
    "\n",
    "    print(\"âš ï¸ No valid target column identified!\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_best_forecasting_feature(tables, user_query, llm, table_names, columns_schema):\n",
    "    table_name, target_column = extract_target_from_query(user_query, llm, table_names, columns_schema)\n",
    "\n",
    "    if not table_name or not target_column:\n",
    "        return \" Target column not found.\", None, None\n",
    "\n",
    "    final_results = []\n",
    "    for name, df in tables.items():\n",
    "        if name != table_name:\n",
    "            continue\n",
    "\n",
    "        related_features = compute_statistical_relation(df, target_column)\n",
    "        for feature, score in related_features:\n",
    "            final_results.append((name, feature, score))\n",
    "\n",
    "    best_feature = sorted(final_results, key=lambda x: x[2], reverse=True)[0] if final_results else None\n",
    "    return best_feature, table_name, target_column\n",
    "\n",
    "user_query = \"I want to forecast future sales trends.\"\n",
    "best_feature, table_name, target_column = get_best_forecasting_feature(tables, user_query, llm, table_names, columns_schema)\n",
    "\n",
    "if best_feature:\n",
    "    print(f\" Best Feature for Forecasting: {best_feature[1]} in {best_feature[0]} with score {best_feature[2]}\")\n",
    "    forecasting_results = forecasting_statistical_tests(tables[table_name], target_column)\n",
    "    print(\"Forecasting Statistical Tests:\", forecasting_results)\n",
    "else:\n",
    "    print(\" No suitable feature found for forecasting.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Statistical  Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
