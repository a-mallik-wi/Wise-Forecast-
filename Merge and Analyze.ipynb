{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e57ce04c-9995-4abd-a57e-39aeb3126a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain-community databricks-sql-connector databricks_langchain openai databricks-sqlalchemy~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91c677d-b428-43ff-9b5d-f02809acf7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5e3838-4fc4-4999-a9b6-4c7ff96997ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain import OpenAI\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7153f997-60f9-4a43-ab4b-40da2777bc6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TimeSeriesAnalysis\").getOrCreate()\n",
    "try:\n",
    "    db = SQLDatabase.from_databricks(catalog=\"forecast\", schema=\"schema1\", engine_args={\"pool_pre_ping\": True})\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LangChain DB: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f91c262-6c47-445f-aaed-2677f10b7725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=800,\n",
    ")\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "agent = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5bd45d9-b2e8-453e-a5fe-ad89e4eee3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "extract_catalog_schema_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant that extracts catalog and schema names from a database query.\n",
    "    The user has provided the following query: \"{input}\"\n",
    "\n",
    "    Please extract the catalog and schema mentioned in the query.\n",
    "    If either the catalog or schema is not provided, return 'Not provided' for the missing one.\n",
    "    Format the response like: Catalog: <catalog_name>, Schema: <schema_name>.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def process_user_query(user_query):\n",
    "    catalog_schema_chain = LLMChain(prompt=extract_catalog_schema_prompt, llm=llm)\n",
    "    catalog_schema_response = catalog_schema_chain.invoke({\"input\": user_query})\n",
    "    # Extract catalog and schema from the response\n",
    "    catalog, schema = 'Not provided', 'Not provided'\n",
    "    if 'Catalog:' in catalog_schema_response['text'] and 'Schema:' in catalog_schema_response['text']:\n",
    "        catalog = catalog_schema_response['text'].split('Catalog:')[1].split(',')[0].strip()\n",
    "        schema = catalog_schema_response['text'].split('Schema:')[1].strip()\n",
    "    return catalog.replace('.', ''), schema.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790d9aeb-c946-43f1-b4e9-4d416d5f528d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_query = \"connect to the forecast catalog and schema1 schema\"\n",
    "catalog, schema= process_user_query(user_query)\n",
    "print(\"Extracted Catalog:\", catalog)\n",
    "print(\"Extracted Schema:\", schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c479472b-6982-4e7b-9e2c-3668ecfcb4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the database connection uses the extracted catalog and schema\n",
    "db = SQLDatabase.from_databricks(\n",
    "    catalog=catalog,\n",
    "    schema=schema,\n",
    "    engine_args={\"pool_pre_ping\": True}  # Prevent session expiration issues\n",
    ")\n",
    "\n",
    "# Fetch table names\n",
    "try:\n",
    "    tables = db.get_usable_table_names()\n",
    "    print(f\"Tables in {catalog}.{schema}: {tables}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving tables from {catalog}.{schema}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f176c4b-0954-4a0e-8e09-b440cae2faa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        spark_df = spark.table(f\"forecast.schema1.{table}\")\n",
    "        pandas_df = spark_df.toPandas()\n",
    "        dataframes[table] = pandas_df\n",
    "        print(f\"Successfully retrieved data for table: {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data from {table}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d09414b-1839-4f1f-9525-67f7953f77c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table_name, df in dataframes.items():\n",
    "    print(f\"Data for table: {table_name}\")\n",
    "    print(df.head())  \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5866da-53e1-485f-8d09-dc4b8a95bc32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy, skew, kurtosis\n",
    "\n",
    "def compute_statistics(dataframes):\n",
    "    stats = {}\n",
    "    \n",
    "    for table_name, df in dataframes.items():\n",
    "        table_stats = {}\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            col_stats = {}\n",
    "\n",
    "            if np.issubdtype(col_data.dtype, np.number):\n",
    "                col_stats[\"count\"] = col_data.count()\n",
    "                col_stats[\"mean\"] = col_data.mean()\n",
    "                col_stats[\"median\"] = col_data.median()\n",
    "                col_stats[\"mode\"] = col_data.mode().iloc[0] if not col_data.mode().empty else None\n",
    "                col_stats[\"variance\"] = col_data.var()\n",
    "                col_stats[\"std_dev\"] = col_data.std()\n",
    "                col_stats[\"min\"] = col_data.min()\n",
    "                col_stats[\"max\"] = col_data.max()\n",
    "                col_stats[\"range\"] = col_data.max() - col_data.min()\n",
    "                col_stats[\"iqr\"] = col_data.quantile(0.75) - col_data.quantile(0.25)\n",
    "                col_stats[\"skewness\"] = skew(col_data)\n",
    "                col_stats[\"kurtosis\"] = kurtosis(col_data)\n",
    "                col_stats[\"sum\"] = col_data.sum()\n",
    "            \n",
    "            elif col_data.dtype == 'bool':\n",
    "                col_stats[\"count\"] = col_data.count()\n",
    "                col_stats[\"true_count\"] = col_data.sum()\n",
    "                col_stats[\"false_count\"] = len(col_data) - col_data.sum()\n",
    "                col_stats[\"true_ratio\"] = col_data.mean()\n",
    "\n",
    "            elif col_data.dtype == 'object' or col_data.dtype.name == 'category':\n",
    "                col_stats[\"count\"] = col_data.count()\n",
    "                col_stats[\"unique\"] = col_data.nunique()\n",
    "                col_stats[\"mode\"] = col_data.mode().iloc[0] if not col_data.mode().empty else None\n",
    "                value_counts = col_data.value_counts(normalize=True)\n",
    "                col_stats[\"entropy\"] = entropy(value_counts, base=2) if not value_counts.empty else None\n",
    "\n",
    "            elif np.issubdtype(col_data.dtype, np.datetime64):\n",
    "                col_stats[\"count\"] = col_data.count()\n",
    "                col_stats[\"min_date\"] = col_data.min()\n",
    "                col_stats[\"max_date\"] = col_data.max()\n",
    "                col_stats[\"range_days\"] = (col_data.max() - col_data.min()).days\n",
    "\n",
    "            table_stats[col] = col_stats\n",
    "        \n",
    "        stats[table_name] = table_stats\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a75a86-2f46-4fe6-8160-c460a4ee3ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "stats = compute_statistics(dataframes)\n",
    "table_name = \"calendar\"  \n",
    "pprint.pprint(stats[table_name])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d572db47-c138-4a60-8319-17fa2739b676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(\n",
    "    {(table, col): stats[table][col] for table in stats for col in stats[table]}\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d260655-66f1-42f3-8fbe-a7d6b4f78f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ff23ce-19ee-4d4e-9252-74db6bbf3a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "stats_df = stats_df.fillna(np.nan)\n",
    "\n",
    "for col in stats_df.select_dtypes(include=[np.number]).columns:\n",
    "    stats_df[col] = stats_df[col].astype(float)\n",
    "\n",
    "for col in stats_df.select_dtypes(include=[object]).columns:\n",
    "    stats_df[col] = stats_df[col].astype(str)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark_df = spark.createDataFrame(stats_df)\n",
    "catalog = \"forecast\"\n",
    "schema = \"schema1\"\n",
    "table_name = \"stats_table\"\n",
    "\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00f5388-dc77-4f57-950f-33751cedd195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatDatabricks\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3545fd61-6b42-4aea-bc1c-9c598119b866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_schema(table_name):\n",
    "    query = f\"DESCRIBE {table_name}\"\n",
    "    result = db.run(query) \n",
    "    return result\n",
    "\n",
    "# Step 2: Provide the LLM with a prompt to decide on the best merge strategy\n",
    "merge_strategy_prompt = PromptTemplate(\n",
    "    input_variables=[\"tables_schema\"],\n",
    "    template=\"\"\"\n",
    "    The user has provided the schema information of the following tables:\n",
    "\n",
    "    {tables_schema}\n",
    "\n",
    "    Based on the provided schemas, suggest the best strategy for merging these tables. \n",
    "    Consider factors such as common columns, column data types, and potential keys for joining.\n",
    "    All tables are stored in 'dataframes' which refers to a dictionary where the keys are the table names (as strings), and the values are the corresponding Pandas DataFrames that hold the data from those tables.\n",
    "    Merge the tables as a pandas DataFrame by providing the appropriate python code.\n",
    "    Make merges stepwise, only providing the necessary code and little explanation for each step.\n",
    "    Make sure the column datatypes are correctly matched when merging.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def generate_merge_strategy(tables):\n",
    "    tables_schema = \"\"\n",
    "    for table in tables:\n",
    "        schema = get_table_schema(table)\n",
    "        tables_schema += f\"Table {table}: {schema}\\n\\n\"\n",
    "    \n",
    "    merge_strategy_chain = LLMChain(prompt=merge_strategy_prompt, llm=llm)\n",
    "    merge_strategy_response = merge_strategy_chain.invoke({\"tables_schema\": tables_schema})\n",
    "    return merge_strategy_response['text']\n",
    "\n",
    "merge_strategy = generate_merge_strategy(tables)\n",
    "\n",
    "print(\"Recommended Merge Strategy:\")\n",
    "print(merge_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4bb5c63-c2f3-40a9-a835-a7bd5be02f7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    print(f\"Schema for table: {table}\")\n",
    "    \n",
    "    # Get schema using the existing get_table_schema function\n",
    "    display_data = get_table_schema(table)\n",
    "    \n",
    "    # If display_data is a string, evaluate it to a list of tuples\n",
    "    if isinstance(display_data, str):\n",
    "        import ast\n",
    "        display_data = ast.literal_eval(display_data)  # Converts string to list of tuples\n",
    "    \n",
    "    # Print the schema in a grid format\n",
    "    for item in display_data:\n",
    "        print(f\"Column Name: {item[0]}, Datatype: {item[1]}, Comment: {item[2]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7a95e3-2cce-49b1-8c47-f8ae879e4de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_calendar = dataframes['calendar']\n",
    "df_hobbies_1_test_0 = dataframes['hobbies_1_test_0']\n",
    "df_merged_1 = pd.merge(df_calendar, df_hobbies_1_test_0, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46153c61-ad4e-4ac1-a952-af1a76b97f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sales_train_evaluation = dataframes['sales_train_evaluation']\n",
    "\n",
    "print(df_merged_1[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].dtypes)\n",
    "print(df_sales_train_evaluation[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].dtypes)\n",
    "\n",
    "# # Merge on 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id' columns\n",
    "# df_merged_2 = pd.merge(df_merged_1, df_sales_train_evaluation, on=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285607a9-597c-4259-8112-6fd711ce5545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Merge and Analyze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
