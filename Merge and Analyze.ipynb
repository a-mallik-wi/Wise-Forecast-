{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e57ce04c-9995-4abd-a57e-39aeb3126a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain-community databricks-sql-connector databricks_langchain openai databricks-sqlalchemy~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91c677d-b428-43ff-9b5d-f02809acf7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5e3838-4fc4-4999-a9b6-4c7ff96997ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain import OpenAI\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7153f997-60f9-4a43-ab4b-40da2777bc6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TimeSeriesAnalysis\").getOrCreate()\n",
    "try:\n",
    "    db = SQLDatabase.from_databricks(catalog=\"forecast\", schema=\"schema1\", engine_args={\"pool_pre_ping\": True})\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LangChain DB: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f91c262-6c47-445f-aaed-2677f10b7725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=800,\n",
    ")\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "agent = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5bd45d9-b2e8-453e-a5fe-ad89e4eee3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "extract_catalog_schema_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant that extracts catalog and schema names from a database query.\n",
    "    The user has provided the following query: \"{input}\"\n",
    "\n",
    "    Please extract the catalog and schema mentioned in the query.\n",
    "    If either the catalog or schema is not provided, return 'Not provided' for the missing one.\n",
    "    Format the response like: Catalog: <catalog_name>, Schema: <schema_name>.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def process_user_query(user_query):\n",
    "    catalog_schema_chain = LLMChain(prompt=extract_catalog_schema_prompt, llm=llm)\n",
    "    catalog_schema_response = catalog_schema_chain.invoke({\"input\": user_query})\n",
    "    # Extract catalog and schema from the response\n",
    "    catalog, schema = 'Not provided', 'Not provided'\n",
    "    if 'Catalog:' in catalog_schema_response['text'] and 'Schema:' in catalog_schema_response['text']:\n",
    "        catalog = catalog_schema_response['text'].split('Catalog:')[1].split(',')[0].strip()\n",
    "        schema = catalog_schema_response['text'].split('Schema:')[1].strip()\n",
    "    return catalog.replace('.', ''), schema.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790d9aeb-c946-43f1-b4e9-4d416d5f528d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_query = \"connect to the forecast catalog and schema1 schema\"\n",
    "catalog, schema= process_user_query(user_query)\n",
    "print(\"Extracted Catalog:\", catalog)\n",
    "print(\"Extracted Schema:\", schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c479472b-6982-4e7b-9e2c-3668ecfcb4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the database connection uses the extracted catalog and schema\n",
    "db = SQLDatabase.from_databricks(\n",
    "    catalog=catalog,\n",
    "    schema=schema,\n",
    "    engine_args={\"pool_pre_ping\": True}  # Prevent session expiration issues\n",
    ")\n",
    "\n",
    "# Fetch table names\n",
    "try:\n",
    "    tables = db.get_usable_table_names()\n",
    "    print(f\"Tables in {catalog}.{schema}: {tables}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving tables from {catalog}.{schema}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f176c4b-0954-4a0e-8e09-b440cae2faa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        spark_df = spark.table(f\"forecast.schema1.{table}\")\n",
    "        pandas_df = spark_df.toPandas()\n",
    "        dataframes[table] = pandas_df\n",
    "        print(f\"Successfully retrieved data for table: {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data from {table}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d09414b-1839-4f1f-9525-67f7953f77c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table_name, df in dataframes.items():\n",
    "    print(f\"Data for table: {table_name}\")\n",
    "    print(df.head())  \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb12bdfe-9341-41a4-b1c4-4b761ae65da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table_name, df in dataframes.items():\n",
    "    df = df.infer_objects()\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        print(f'{table_name},{col},{dtype}')\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5866da-53e1-485f-8d09-dc4b8a95bc32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy, skew, kurtosis\n",
    "\n",
    "def compute_statistics(dataframes):\n",
    "    stats = {}\n",
    "    \n",
    "    for table_name, df in dataframes.items():\n",
    "        df = df.infer_objects()\n",
    "        table_stats = {}\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            col_stats = {}\n",
    "            col_stats[\"table_column\"] = f\"{table_name} | {col}\"\n",
    "            col_stats[\"count\"] = col_data.count()\n",
    "            \n",
    "            \n",
    "            dtype_switch = {\n",
    "                'number': lambda: col_stats.update({\n",
    "                    \"mean\": col_data.mean(),\n",
    "                    \"median\": col_data.median(),\n",
    "                    \"mode\": col_data.mode().iloc[0] if not col_data.mode().empty else None,\n",
    "                    \"variance\": col_data.var(),\n",
    "                    \"std_dev\": col_data.std(),\n",
    "                    \"min\": col_data.min(),\n",
    "                    \"max\": col_data.max(),\n",
    "                    \"range\": col_data.max() - col_data.min(),\n",
    "                    \"iqr\": col_data.quantile(0.75) - col_data.quantile(0.25),\n",
    "                    \"skewness\": skew(col_data),\n",
    "                    \"kurtosis\": kurtosis(col_data),\n",
    "                    \"sum\": col_data.sum()\n",
    "                }),\n",
    "                'bool': lambda: col_stats.update({\n",
    "                    \"true_count\": col_data.sum(),\n",
    "                    \"false_count\": len(col_data) - col_data.sum(),\n",
    "                    \"true_ratio\": col_data.mean()\n",
    "                }),\n",
    "                'category': lambda: col_stats.update({\n",
    "                    \"unique\": col_data.nunique(),\n",
    "                    \"mode\": col_data.mode().iloc[0] if not col_data.mode().empty else None,\n",
    "                    \"entropy\": entropy(col_data.value_counts(normalize=True), base=2) if not col_data.value_counts().empty else None\n",
    "                }),\n",
    "                'boolean_like': lambda: col_stats.update({\n",
    "                    \"true_count\": col_data.str.upper().replace({'YES': True, 'NO': False, '1': True, '0': False}).sum(),\n",
    "                    \"false_count\": len(col_data) - col_data.str.upper().replace({'YES': True, 'NO': False, '1': True, '0': False}).sum(),\n",
    "                    \"true_ratio\": col_data.str.upper().replace({'YES': True, 'NO': False, '1': True, '0': False}).mean()\n",
    "                }),\n",
    "                'datetime': lambda: col_stats.update({\n",
    "                    \"min_date\": col_data.min(),\n",
    "                    \"max_date\": col_data.max(),\n",
    "                    \"range_days\": (col_data.max() - col_data.min()).days\n",
    "                })\n",
    "            }\n",
    "            \n",
    "            dtype_key = 'number' if np.issubdtype(col_data.dtype, np.number) else \\\n",
    "                        'bool' if col_data.dtype == 'bool' else \\\n",
    "                        'category' if col_data.dtype == 'object' or col_data.dtype.name == 'category' else \\\n",
    "                        'boolean_like' if col_data.str.upper().replace({'YES': True, 'NO': False, '1': True, '0': False}).isin([True, False]).all() else \\\n",
    "                        'datetime' if np.issubdtype(col_data.dtype, np.datetime64) else None\n",
    "            \n",
    "            if dtype_key:\n",
    "                dtype_switch[dtype_key]()\n",
    "            \n",
    "            table_stats[col] = col_stats\n",
    "        \n",
    "        stats[table_name] = table_stats\n",
    "    \n",
    "    stats_df = pd.DataFrame(\n",
    "        {(table, col): stats[table][col] for table in stats for col in stats[table]}).T\n",
    "    \n",
    "    return stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a75a86-2f46-4fe6-8160-c460a4ee3ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stats_df = compute_statistics(dataframes)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d260655-66f1-42f3-8fbe-a7d6b4f78f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ff23ce-19ee-4d4e-9252-74db6bbf3a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "stats_df = stats_df.fillna(np.nan)\n",
    "\n",
    "for col in stats_df.select_dtypes(include=[np.number]).columns:\n",
    "    stats_df[col] = stats_df[col].astype(float)\n",
    "\n",
    "for col in stats_df.select_dtypes(include=[object]).columns:\n",
    "    stats_df[col] = stats_df[col].astype(str)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark_df = spark.createDataFrame(stats_df)\n",
    "catalog_name = \"forecast\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.stats_schema\")\n",
    "stats_schema_name = \"stats_schema\"\n",
    "table_name = \"stats_table\"\n",
    "\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{stats_schema_name}.{table_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00f5388-dc77-4f57-950f-33751cedd195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatDatabricks\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4d6dff-452f-4161-bf29-d689eef0102b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_schema(table_name):\n",
    "    query = f\"DESCRIBE {table_name}\"\n",
    "    result = db.run(query) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3545fd61-6b42-4aea-bc1c-9c598119b866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Provide the LLM with a prompt to decide on the best merge strategy\n",
    "merge_strategy_prompt = PromptTemplate(\n",
    "    input_variables=[\"tables_schema\"],\n",
    "    template=\"\"\"\n",
    "    The user has provided the schema information of the following tables:\n",
    "\n",
    "    {table_schema}\n",
    "\n",
    "    Based on the provided schemas, suggest the best strategy for merging these tables. \n",
    "    Consider factors such as common columns, column data types, and potential keys for joining.\n",
    "    All tables are stored in 'dataframes' which refers to a dictionary where the keys are the table names (as strings), and the values are the corresponding Pandas DataFrames that hold the data from those tables.\n",
    "    Merge the tables as a pandas DataFrame by providing the appropriate python code.\n",
    "    Make merges stepwise, only providing the necessary code and little explanation for each step.\n",
    "    Make sure the column datatypes are correctly matched when merging.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def generate_merge_strategy(tables):\n",
    "    tables_schema = \"\"\n",
    "    for table in tables:\n",
    "        schema = get_table_schema(table)\n",
    "        tables_schema += f\"Table {table}: {schema}\\n\\n\"\n",
    "    \n",
    "    merge_strategy_chain = LLMChain(prompt=merge_strategy_prompt, llm=llm)\n",
    "    merge_strategy_response = merge_strategy_chain.invoke({\"table_schema\": tables_schema})\n",
    "    return merge_strategy_response['text']\n",
    "\n",
    "merge_strategy = generate_merge_strategy(tables)\n",
    "\n",
    "print(\"Recommended Merge Strategy:\")\n",
    "print(merge_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4bb5c63-c2f3-40a9-a835-a7bd5be02f7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    print(f\"Schema for table: {table}\")\n",
    "    \n",
    "    display_data = get_table_schema(table)\n",
    "    \n",
    "    if isinstance(display_data, str):\n",
    "        import ast\n",
    "        display_data = ast.literal_eval(display_data) \n",
    "    \n",
    "    for item in display_data:\n",
    "        print(f\"Column Name: {item[0]}, Datatype: {item[1]}, Comment: {item[2]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775e97bf-d5c5-4940-922d-2a87a7dac1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "import re\n",
    "import json\n",
    "\n",
    "tables_schema = \"\"\n",
    "for table in tables:\n",
    "    schema = get_table_schema(table)\n",
    "    tables_schema += f\"Table {table}: {schema}\\n\\n\"\n",
    "\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=8000,\n",
    ")\n",
    "\n",
    "def split_into_chunks(text, max_tokens=500):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if len(current_chunk.split()) + len(line.split()) <= max_tokens:\n",
    "            current_chunk += line + \"\\n\"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = line + \"\\n\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())  \n",
    "    return chunks\n",
    "\n",
    "chunks = split_into_chunks(tables_schema)\n",
    "\n",
    "memory = []\n",
    "previous_features = set()  \n",
    "\n",
    "user_query = \"I want to forecast future sales based on relevant features.\"\n",
    "\n",
    "for chunk in chunks:\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in time series forecasting. The user wants to forecast future values based on relevant features. \n",
    "    Given the following table schema, classify each column as either 'Relevant' or 'Irrelevant' for time series forecasting, and provide a reason for your classification.\n",
    "\n",
    "    {memory}{chunk}\n",
    "\n",
    "    User request: {user_query}\n",
    "\n",
    "    Please provide your response in the following text format:\n",
    "    - \"column1\": \"The column represents time-based data, essential for forecasting.\"\n",
    "    - \"column2\": \"The column contains historical sales data, which is critical for forecasting.\"\n",
    "\n",
    "    IMPORTANT: Please do not repeat any features that have already been mentioned in previous responses. Only mention new relevant features with their reasons.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    llm_response = response.text if hasattr(response, \"text\") else response.content\n",
    "\n",
    "    for line in llm_response.splitlines():\n",
    "        match = re.match(r'- \"(.*?)\": \"(.*?)\"', line.strip())\n",
    "        if match:\n",
    "            feature_name = match.group(1)\n",
    "            if feature_name not in previous_features:\n",
    "                previous_features.add(feature_name)\n",
    "            else:\n",
    "                llm_response = llm_response.replace(line.strip(), \"\")\n",
    "\n",
    "    memory.append(llm_response)\n",
    "\n",
    "final_response = \"\\n\".join(memory)\n",
    "print(\"Final Relevant Features and Reasons:\")\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7a95e3-2cce-49b1-8c47-f8ae879e4de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############### Filtering using Correlation Metrics #####################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "threshold = 0.85\n",
    "\n",
    "filtered_dataframes = {}\n",
    "\n",
    "for table_name, df in dataframes.items():\n",
    "    print(f\"Processing correlation analysis for table: {table_name}\")\n",
    "\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    correlated_features = set()\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                correlated_features.add(correlation_matrix.columns[i])\n",
    "\n",
    "    print(f\"Highly Correlated Features to Remove in {table_name}: {correlated_features}\\n\")\n",
    "    filtered_dataframes[table_name] = df.drop(columns=correlated_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46153c61-ad4e-4ac1-a952-af1a76b97f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table_name, df in filtered_dataframes.items():\n",
    "    print(f\"Filtered Data for table: {table_name}\")\n",
    "    print(df.head()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285607a9-597c-4259-8112-6fd711ce5545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def handle_special_columns(df):\n",
    "    date_columns = df.select_dtypes(include=[np.datetime64]).columns\n",
    "    for col in date_columns:\n",
    "        df[f'{col}_day'] = df[col].dt.day\n",
    "        df[f'{col}_month'] = df[col].dt.month\n",
    "        df[f'{col}_weekday'] = df[col].dt.weekday\n",
    "        df[f'{col}_year'] = df[col].dt.year\n",
    "\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in categorical_columns:\n",
    "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_transformations(df, stats_df, table_name):\n",
    "    for column in df.columns:\n",
    "        table_column = f\"{table_name} | {column}\"\n",
    "\n",
    "        if table_column not in stats_df['table_column'].values:\n",
    "            print(f\"Skipping feature {column} as no statistics found for {table_column}.\")\n",
    "            continue  \n",
    "\n",
    "        stat_row = stats_df[stats_df['table_column'] == table_column].iloc[0]\n",
    "        skewness = stat_row['skewness']\n",
    "        kurtosis_value = stat_row['kurtosis']\n",
    "        entropy_value = stat_row['entropy']\n",
    "\n",
    "        if skewness > 1:\n",
    "            print(f\"Applying log transformation to skewed feature: {column}\")\n",
    "            df[column] = np.log1p(df[column])  \n",
    "\n",
    "        if kurtosis_value > 3:\n",
    "            print(f\"Applying robust scaing to feature: {column}\")\n",
    "            scaler = RobustScaler()\n",
    "            df[column] = scaler.fit_transform(df[column].values.reshape(-1, 1))\n",
    "\n",
    "        if entropy_value < 0.1:\n",
    "            print(f\"Removing low-entropy feature: {column}\")\n",
    "            df = df.drop(columns=[column])\n",
    "\n",
    "    return df\n",
    "\n",
    "for table_name, df in filtered_dataframes.items():\n",
    "    print(f\"Applying transformations to table: {table_name}\")\n",
    "    \n",
    "    df_transformed = apply_transformations(df, stats_df, table_name)\n",
    "    \n",
    "    df_transformed = handle_special_columns(df_transformed)\n",
    "    \n",
    "    print(f\"Transformed data for table: {table_name}\")\n",
    "    print(df_transformed.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Merge and Analyze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
